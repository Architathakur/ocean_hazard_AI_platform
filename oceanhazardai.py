# -*- coding: utf-8 -*-
"""oceanHazardAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ODuqUi0cwdVtW1znTS5L49Ck8CD_2h7M
"""

# Step 0: Install dependencies
!pip install -q torch==2.8.0 transformers==4.34.0 langdetect googletrans==4.0.0-rc1 spacy folium
!python -m spacy download en_core_web_sm

!pip install --upgrade huggingface_hub

# Step 1: Imports & device setup
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚ö° Using device: {device}")

import re
from datetime import datetime
from typing import List, Dict
from transformers import pipeline
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException
from googletrans import Translator
import spacy
import folium

# Load spaCy for location extraction
nlp = spacy.load("en_core_web_sm")

# Step 2: OceanHazardAI class
class OceanHazardAI:
    def __init__(self):
        print("ü§ñ Initializing Ocean Hazard AI Engine...")

        # Sentiment Analyzer
        print("üìù Loading sentiment analysis model...")
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment-latest",
            device=0 if device.type != "cpu" else -1,
            return_all_scores=True
        )

        # Hazard Classifier
        print("üéØ Loading hazard classification model...")
        self.classifier = pipeline(
            "zero-shot-classification",
            model="valhalla/distilbart-mnli-12-1",
            device=0 if device.type != "cpu" else -1
        )

        # Translator
        print("üåç Loading translation service...")
        self.translator = Translator()

        # Hazard labels
        self.hazard_labels = [
            "tsunami", "high waves", "flooding", "storm surge",
            "coastal erosion", "dangerous currents", "cyclone",
            "abnormal tides", "sea level rise", "normal weather"
        ]

        # Urgency keywords
        self.urgency_keywords = {
            'high': ['emergency','danger','evacuate','urgent','help','disaster','massive','huge','destroy','death','‡§ñ‡§§‡§∞‡§æ','‡§Ü‡§™‡§æ‡§§‡§ï‡§æ‡§≤','‡§¨‡§ö‡§æ‡§ì','‡§Æ‡§¶‡§¶','‡ÆÜ‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ','‡Æâ‡Æ§‡Æµ‡Æø','‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ','‡∞™‡±ç‡∞∞‡∞Æ‡∞æ‡∞¶‡∞Ç','‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç','‡∞ï‡∞æ‡∞™‡∞æ‡∞°‡±Å'],
            'medium': ['concern','worry','alert','warning','rising','‡§ö‡§ø‡§Ç‡§§‡§æ','‡§∏‡§æ‡§µ‡§ß‡§æ‡§®','‡§ö‡•á‡§§‡§æ‡§µ‡§®‡•Ä','‡Æï‡Æµ‡Æ≤‡Øà','‡Æé‡Æö‡Øç‡Æö‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Øà','‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞®','‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞ï'],
            'low': ['normal','calm','peaceful','fine','good','‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø','‡§∂‡§æ‡§Ç‡§§','‡Æö‡Ææ‡Æ§‡Ææ‡Æ∞‡Æ£','‡ÆÖ‡ÆÆ‡Øà‡Æ§‡Æø','‡∞∏‡∞æ‡∞ß‡∞æ‡∞∞‡∞£','‡∞∂‡∞æ‡∞Ç‡∞§‡∞ø']
        }

        # Coastal city regex fallback
        self.coastal_cities = [
            'mumbai','chennai','kolkata','kochi','goa','visakhapatnam','mangalore','puducherry','thiruvananthapuram','bhubaneswar',
            'surat','vadodara','rajkot','jamnagar','dwarka','somnath','puri','paradip','haldia','kakinada','machilipatnam','nellore','ongole','guntur','vijayawada','rajahmundry'
        ]
        print("‚úÖ AI Engine initialized successfully!")

    # Language detection & translation
    def detect_language(self, text: str) -> str:
        try: return detect(text)
        except LangDetectError: return 'en'

    def translate_text(self, text: str, target_lang='en') -> Dict:
        try:
            src_lang = self.detect_language(text)
            if src_lang == target_lang: return {'original': text,'translated': text,'source_language': src_lang}
            translated = self.translator.translate(text, dest=target_lang)
            return {'original': text,'translated': translated.text,'source_language': src_lang}
        except: return {'original': text,'translated': text,'source_language':'unknown'}

    # Location extraction
    def extract_location(self, text: str) -> str:
        doc = nlp(text)
        for ent in doc.ents:
            if ent.label_ == "GPE": return ent.text
        text_lower = text.lower()
        for city in self.coastal_cities:
            if city in text_lower: return city.title()
        return "Unknown"

    # Urgency calculation
    def calculate_urgency(self, text: str, sentiment_scores: List) -> str:
        text_lower = text.lower()
        score = sum(3 for w in self.urgency_keywords['high'] if w in text_lower)
        score += sum(2 for w in self.urgency_keywords['medium'] if w in text_lower)
        score -= sum(1 for w in self.urgency_keywords['low'] if w in text_lower)
        negative_score = next((s['score'] for s in sentiment_scores[0] if s['label']=='LABEL_0'),0)
        if score >= 3 or negative_score>0.7: return "HIGH"
        elif score>=1 or negative_score>0.5: return "MEDIUM"
        else: return "LOW"

    # Analyze single post
    def analyze_post(self, text: str, location: str = None) -> Dict:
        tr = self.translate_text(text)
        txt = tr['translated']
        sentiment_results = self.sentiment_analyzer(txt)
        top_sentiment = max(sentiment_results[0], key=lambda x:x['score'])
        hazard_result = self.classifier(txt, self.hazard_labels)
        if location is None: location = self.extract_location(text)
        urgency = self.calculate_urgency(txt, sentiment_results)
        return {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'original_text': text,
            'translated_text': txt if tr['source_language']!='en' else None,
            'language': tr['source_language'],
            'location': location,
            'hazard_type': hazard_result['labels'][0],
            'hazard_confidence': round(hazard_result['scores'][0],3),
            'sentiment': top_sentiment['label'],
            'sentiment_score': round(top_sentiment['score'],3),
            'urgency_level': urgency,
            'all_hazard_scores': {l:round(s,3) for l,s in zip(hazard_result['labels'],hazard_result['scores'])}
        }

    # Batch analysis
    def batch_analyze(self, posts: List[Dict]) -> List[Dict]:
        results=[]
        for post in posts:
            text = post.get('text','')
            loc = post.get('location',None)
            if text.strip(): results.append(self.analyze_post(text, loc))
        return results

    # Hotspot generation
    def generate_hotspots(self, analysis_results: List[Dict], min_reports: int = 2) -> List[Dict]:
        loc_data = {}
        for res in analysis_results:
            loc = res['location']
            if loc=="Unknown": continue
            if loc not in loc_data:
                loc_data[loc] = {'total_reports':0,'high_urgency':0,'medium_urgency':0,'hazard_types':{},'avg_confidence':0,'latest_report':res['timestamp']}
            d = loc_data[loc]
            d['total_reports']+=1
            if res['urgency_level']=='HIGH': d['high_urgency']+=1
            elif res['urgency_level']=='MEDIUM': d['medium_urgency']+=1
            h = res['hazard_type']
            d['hazard_types'][h] = d['hazard_types'].get(h,0)+1
            d['avg_confidence']+=res['hazard_confidence']
            if res['timestamp']>d['latest_report']: d['latest_report']=res['timestamp']
        hotspots=[]
        for loc,d in loc_data.items():
            if d['total_reports']>=min_reports:
                d['avg_confidence']/=d['total_reports']
                if d['high_urgency']>0: urgency='HIGH'
                elif d['medium_urgency']>0: urgency='MEDIUM'
                else: urgency='LOW'
                primary_hazard=max(d['hazard_types'].items(),key=lambda x:x[1])
                hotspots.append({
                    'location':loc,'total_reports':d['total_reports'],'urgency_level':urgency,
                    'primary_hazard':primary_hazard[0],'hazard_count':primary_hazard[1],
                    'avg_confidence':round(d['avg_confidence'],3),
                    'high_urgency_reports':d['high_urgency'],
                    'medium_urgency_reports':d['medium_urgency'],
                    'latest_report':d['latest_report'],
                    'all_hazards':d['hazard_types']
                })
        return sorted(hotspots,key=lambda x:( {'HIGH':3,'MEDIUM':2,'LOW':1}[x['urgency_level']],x['total_reports']),reverse=True)

# Step 3: Test posts
ai_engine = OceanHazardAI()

test_posts = [
    {'text': "Massive tsunami waves hitting Marina Beach Chennai! People running! #emergency #tsunami"},
    {'text': "‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§∞‡§ø‡§∂, ‡§∏‡§°‡§º‡§ï‡•ã‡§Ç ‡§™‡§∞ ‡§™‡§æ‡§®‡•Ä ‡§≠‡§∞ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§∏‡§≠‡•Ä ‡§∏‡§æ‡§µ‡§ß‡§æ‡§® ‡§∞‡§π‡•á‡§Ç!"},
    {'text': "Beautiful sunset at Goa beach today. Weather is perfect for swimming!"},
    {'text': "‡ÆÜ‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ! Chennai harbor near high tide. Warning issued!"},
    {'text': "Sunny day at Goa beach, tourists enjoying."},
    {'text': "Massive flooding in Mumbai streets after heavy rainfall. Rescue teams deployed."},
]

results = ai_engine.batch_analyze(test_posts)

# Step 4: Print results
for i,res in enumerate(results):
    print(f"\n--- Post {i+1} ---\n{res}")

import pandas as pd

# Convert results into DataFrame
df = pd.DataFrame(results)
df_display = df[['original_text','location','hazard_type','hazard_confidence','sentiment','urgency_level']]

# Apply styling
def highlight_urgency(val):
    if val == 'HIGH':
        return 'background-color: red; color: white; font-weight: bold'
    elif val == 'MEDIUM':
        return 'background-color: orange; color: black; font-weight: bold'
    else:
        return 'background-color: green; color: white'

def bold_text(val):
    return 'font-weight: bold'

styled_df = df_display.style.applymap(highlight_urgency, subset=['urgency_level'])\
                            .applymap(bold_text, subset=['hazard_type','sentiment'])\
                            .set_properties(**{'text-align': 'left'})\
                            .set_table_styles([{
                                'selector': 'th',
                                'props': [('background-color', '#40466e'),
                                          ('color', 'white'),
                                          ('font-size', '14px'),
                                          ('text-align', 'center')]
                            }])

styled_df

import plotly.express as px

fig = px.data.tips()  # Dummy example; replace with your df_display
import plotly.graph_objects as go

fig = go.Figure(data=[go.Table(
    header=dict(values=list(df_display.columns),
                fill_color='darkblue',
                font=dict(color='white', size=14),
                align='left'),
    cells=dict(values=[df_display[col] for col in df_display.columns],
               fill_color=[['lightcoral' if lvl=='HIGH' else 'lightyellow' if lvl=='MEDIUM' else 'lightgreen'
                            for lvl in df_display['urgency_level']] for _ in df_display.columns],
               align='left'))
])

fig.show()

# Step 5: Generate hotspots
hotspots = ai_engine.generate_hotspots(results)
print(f"\nüö® Hotspots found: {len(hotspots)}")
for h in hotspots: print(h)

# Step 6: Optional map visualization
import folium

city_coords = {"Chennai":[13.0827,80.2707],"Mumbai":[19.0760,72.8777],"Goa":[15.2993,74.1240]}
m = folium.Map(location=[20,77], zoom_start=5)

for h in hotspots:
    loc = h['location']
    if loc in city_coords:
        folium.CircleMarker(
            location=city_coords[loc],
            radius=5 + h['avg_confidence']*10,
            popup=f"{loc}\nHazard: {h['primary_hazard']}\nUrgency: {h['urgency_level']}",
            color='red' if h['urgency_level']=='HIGH' else ('orange' if h['urgency_level']=='MEDIUM' else 'green'),
            fill=True
        ).add_to(m)

m

import matplotlib.pyplot as plt
from collections import Counter

hazards = [res['hazard_type'] for res in results]
counts = Counter(hazards)
plt.bar(counts.keys(), counts.values(), color='skyblue')
plt.title("Hazard Types Detected")
plt.ylabel("Number of Posts")
plt.xticks(rotation=45)
plt.show()





